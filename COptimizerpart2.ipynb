{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fatymazahrae/DataCleanOptimizer/blob/main/COptimizerpart2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7f8d0c9",
      "metadata": {
        "id": "f7f8d0c9"
      },
      "source": [
        "## Project Guide  \n",
        "------------  \n",
        "- [Project Overview](#project-overview)  \n",
        "- [Part 1: Reading Data - Exploratory Data Analysis](#I)\n",
        "- [Part 2: Visual data analysis](#II)\n",
        "- [Part 3: Data Pre-processing &  Preparation](#III)\n",
        "- [Part 4: process automation](#IV)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8aa62351",
      "metadata": {
        "id": "8aa62351"
      },
      "source": [
        "<a id=\"project-overview\"></a>\n",
        "\n",
        "# Project Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3bae56e",
      "metadata": {
        "id": "b3bae56e"
      },
      "source": [
        "![image.png](attachment:image.png)\n",
        "\n",
        "##### This project aims to optimize data cleaning and preprocessing algorithms to prepare high-quality datasets for subsequent analyses. The optimization focuses not only on the efficiency of the algorithms but also on their ability to handle large volumes of data while maintaining informationÂ quality.\n",
        "\n",
        "### about dataset :\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0jC0ilSy6_aV",
      "metadata": {
        "id": "0jC0ilSy6_aV"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47221d51",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "47221d51"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import dask.dataframe as dd\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mOg4BEho8k-a",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "mOg4BEho8k-a"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zqXNfyGj9nBy",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zqXNfyGj9nBy"
      },
      "outputs": [],
      "source": [
        "import dask.dataframe as dd\n",
        "\n",
        "# Path to your CSV file in Google Drive\n",
        "file_path = '/content/drive/My Drive/adult.csv'\n",
        "\n",
        "# Load the data using Dask (similar to pd.read_csv for Pandas)\n",
        "df = dd.read_csv(file_path)\n",
        "\n",
        "# To get a quick look at the data, you can compute a small sample\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Zyuc_r6EBz2b",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Zyuc_r6EBz2b"
      },
      "outputs": [],
      "source": [
        "df.compute().describe()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.compute().shape"
      ],
      "metadata": {
        "id": "gVoIdqm0P8nz"
      },
      "id": "gVoIdqm0P8nz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "601f2e27",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "601f2e27"
      },
      "outputs": [],
      "source": [
        "import hashlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0mcbU9GaBZEu",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0mcbU9GaBZEu"
      },
      "outputs": [],
      "source": [
        "# Define the columns used for near-duplicate detection\n",
        "NEAR_DUPLICATES_COLUMNS = ['workclass', 'fnlwgt', 'education', 'education-num',\n",
        "                           'marital-status', 'occupation', 'relationship', 'race', 'sex',\n",
        "                           'capital-gain', 'capital-loss', 'hours-per-week', 'native-country']\n",
        "\n",
        "def compute_hash(row):\n",
        "    available_cols = [col for col in NEAR_DUPLICATES_COLUMNS if col in row.index]\n",
        "    row_str = ''.join([str(row[col]) for col in available_cols])\n",
        "    return hashlib.sha256(row_str.encode('utf-8')).hexdigest()\n",
        "\n",
        "def process_partition(partition):\n",
        "    return partition.apply(compute_hash, axis=1)\n",
        "\n",
        "# Provide meta information to map_partitions\n",
        "meta = pd.Series(dtype=str, name='hash_value')\n",
        "df['hash_value'] = df.map_partitions(process_partition, meta=meta)\n",
        "\n",
        "df = df.drop_duplicates(subset=['hash_value'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pKYEUhmOHPHB",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "pKYEUhmOHPHB"
      },
      "outputs": [],
      "source": [
        "df.compute().shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rZGu5jRoHDEL",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rZGu5jRoHDEL"
      },
      "outputs": [],
      "source": [
        "df = df.drop('hash_value', axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a78377c",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6a78377c"
      },
      "outputs": [],
      "source": [
        "def detect_dtype(data):\n",
        "    num_features = []\n",
        "    cat_features = []\n",
        "\n",
        "    for col in data.columns:\n",
        "        if data[col].dtype.kind in 'biufc':\n",
        "            num_features.append(col)\n",
        "        else:\n",
        "            cat_features.append(col)\n",
        "\n",
        "    return num_features, cat_features\n",
        "\n",
        "num_features, cat_features = detect_dtype(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4452dff5",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4452dff5"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "for ftr in cat_features:\n",
        "    df[ftr] = df[ftr].fillna('missing')\n",
        "\n",
        "mean_values = df[num_features].mean().compute()\n",
        "df[num_features] = df[num_features].fillna(mean_values)\n",
        "\n",
        "data_sample = df[num_features].sample(frac=0.1).compute()\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(data_sample)\n",
        "\n",
        "df[num_features] = df.map_partitions(lambda partition: pd.DataFrame(scaler.transform(partition[num_features]),\n",
        "                                                                    columns=num_features))\n",
        "label_encoders = {}\n",
        "\n",
        "def label_encode_partition(partition, encoders):\n",
        "    for col in cat_features:\n",
        "        encoder = encoders[col]\n",
        "        partition[col] = encoder.fit_transform(partition[col])\n",
        "    return partition\n",
        "\n",
        "# Create LabelEncoders for each categorical column\n",
        "label_encoders = {col: LabelEncoder() for col in cat_features}\n",
        "\n",
        "# Apply the label encoding partition by partition\n",
        "df = df.map_partitions(label_encode_partition, encoders=label_encoders, meta=df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "y5a4pnO3CqVT",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "y5a4pnO3CqVT"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Input, Embedding, Flatten, Dense, concatenate\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "num_input = Input(shape=(len(num_features),), name='num_input')\n",
        "\n",
        "def embed_ftr(ftr):\n",
        "    cat_input = Input(shape=(1,), name=f'cat_input_{ftr}')\n",
        "    cat_embed = Embedding(input_dim=len(df[ftr].unique()), output_dim=2, input_length=1)(cat_input)\n",
        "    cat_flatten = Flatten()(cat_embed)\n",
        "    return cat_input, cat_flatten\n",
        "\n",
        "embedded_cats = []\n",
        "cat_inputs = []\n",
        "\n",
        "for ftr in cat_features:\n",
        "    cat_input, embedded_cat = embed_ftr(ftr)\n",
        "    embedded_cats.append(embedded_cat)\n",
        "    cat_inputs.append(cat_input)\n",
        "\n",
        "# # Combine numerical input and embedded categorical features\n",
        "# combined = concatenate([num_input] + embedded_cats)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}